{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "<font color='red'>This project utilizes Spark, S3, and Redshift to load, extract and transform data about immigration to the U.S. This project will combine what the lessons and skills gained throughout the program. <br>\n",
    "    At a high level, this project preprocesses the immigration and supporting data using PySpark and staging it into S3. From S3, the data is staged into Redshift and then transformed into fact and dimension tables. Finally, data quality checks are performed to ensure the entire pipeline ran correctly.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc> <br>\n",
    "<font color='red'>\n",
    "    This project will load in data from the SAS files containing I94 immigration data, temperatures by city data, demographic data, and airport data. Other data used in this project is the information from the i94 description file. Each of these datasets are preprocessed and cleaned in order to be loaded into S3. Once the data is loaded into S3 the data is then copied into Redshift in staging tables. Once the staging tables are loaded into Redshift, the data is then extracted and transformed to create the fact and dimension tables. The data ends in the Redshift database to be used and accessed for further analysis. Besides Pyspark, S3, and Redshift, other tools used in this project are Jupyter Notebooks and AWS Console.\n",
    "    </font>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>\n",
    "<h3>The main dataset will include data on immigration to the United States, and supplementary datasets will include data on airport codes, U.S. city demographics, and temperature data. The data sets include the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>\n",
    "\n",
    "<h4>I94 Immigration Data</h4><br>\n",
    "    This data comes from the US National Tourism and Trade Office. This data is stored as a set of SAS7BDAT files. SAS7BDAT is a database storage file created by Statistical Analysis System (SAS) software to store data. A separate SAS file `I94_SAS_Labels_Descriptions.SAS` is also provided to describe the data found in the source data. This description also provides the port and country codes mapped to the decoded strings. This data is the source of most of the data in the data model and represents 12 months of data for the year 2016. <br>\n",
    "This data is initially processed using spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " <font color='red'>\n",
    "    <h4>World Temperature Data</h4><br>\n",
    "    This CSV dataset contains the recorded temperatures by city from 1743-11-01 to 2013-09-01. In order to select temperatures that were relevant to our dataset, I chose the most recent recording of each city by sorting and dropping duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color=\"red\">\n",
    "<h4>U.S. City Demographic Data</h4><br>\n",
    "    This CSV data contains information about the demographics of the U.S. This data will support the port data to gather more information about the demographic at each port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "<h4>Airport Code Table</h4><br>\n",
    "    This is a simple CSV table of airport codes and corresponding cities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'><b>\n",
    "Preprocessing and data cleaning are performed in `preprocessing.py` and included in the data pipeline `pipeline.py`\n",
    "<br><br>\n",
    "A total of 6 data sets were cleaned, processed, and written to S3. <br>\n",
    "The following functions were used to clean and process each of the 6 datasets: \n",
    "    <ol type = \"1\">\n",
    "         <li>`process_port_codes` Process the port codes, city, and state found in I94_SAS_Labels_Descriptions.SAS </li>\n",
    "         <li>`process_country_codes` Process the country codes and respective countries found in I94_SAS_Labels_Descriptions.SAS</li>\n",
    "         <li>`process_airport_codes` Process airport data found in data/raw_data/airport-codes_csv</li>\n",
    "         <li>`process_temperature` Process temperature data found in data/raw_data/GlobalLandTemperaturesByCity.csv, clean column names, and choose most recent average temperatures</li>\n",
    "         <li>`process_demographic` Process demographic data found in data/raw_data/us-cities-demographics.csv and clean column names</li>\n",
    "         <li>`process_immigration_data`</li> from 2016 found in ../../data/18-83510-I94-Data-2016/, decode visa types, modes, and datetimes\n",
    "      </ol>\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PDF(object):\n",
    "    def __init__(self, pdf, size=(200,200)):\n",
    "        self.pdf = pdf\n",
    "        self.size = size\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '<iframe src={0} width={1[0]} height={1[1]}></iframe>'.format(self.pdf, self.size)\n",
    "\n",
    "    def _repr_latex_(self):\n",
    "        return r'\\includegraphics[width=1.0\\textwidth]{{{0}}}'.format(self.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>\n",
    "    For this project I used a snowflake schema which has one fact table with multiple dimension tables to support the fact table data. Since there was one main source of data, the other data was able to support the main source. Below is the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=capstone.pdf width=1000 height=1000></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{capstone.pdf}"
      ],
      "text/plain": [
       "<__main__.PDF at 0x7f7020227438>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF('capstone.pdf',size=(1000,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'><b>\n",
    "The necessary steps for the pipeline are performed in `pipeline.py`\n",
    "<br><br>\n",
    "The steps performed are: <br>\n",
    "Preprocess data --> Stage data --> Extract data --> Check Data Quality\n",
    "    <ol type = \"1\">\n",
    "         <li>Preprocess Data `preprocessing.py` : Data is processed from the raw data using Spark and then written to S3 </li>\n",
    "         <li>Stage Data `stage_data.py` : Data is copied from S3 into Redshift staging tables</li>\n",
    "         <li>Extract Data `extract_data.py` : Data is extracted from staging tables into fact and dimesion tables</li>\n",
    "         <li>Check Data Quality `quality_check.py` : Data quality is checked after the extracted data is organized</li>\n",
    "    </ol>\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'><b>\n",
    "The data pipeline is created in `pipeline.py`\n",
    "<br>\n",
    "To run the pipeline, use the following command in the terminal\n",
    "<br>\n",
    "```\n",
    "python pipeline.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>\n",
    "Test decoding functions from `preprocessing.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode Visa Passed: True\n",
      "Decode Mode Passed: True\n",
      "Convert Time Passed: True\n",
      "SAS Day Passed: True\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import decode_i94visa, decode_mode, convert_sas_datetime, get_sas_day\n",
    "from datetime import datetime\n",
    "\n",
    "expected_visa = 'Business'\n",
    "print(f'Decode Visa Passed: {decode_i94visa(1) == expected_visa}')\n",
    "\n",
    "expected_mode = 'Air'\n",
    "print(f'Decode Mode Passed: {decode_mode(1) == expected_mode}')\n",
    "\n",
    "expected_datetime = datetime(2016, 4, 7, 0, 0)\n",
    "print(f'Convert Time Passed: {convert_sas_datetime(20551) == expected_datetime}')\n",
    "\n",
    "expected_day = 7\n",
    "print(f'SAS Day Passed: {get_sas_day(20551) == expected_day}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'><b>\n",
    "Data quality Checks are performed in `quality_check.py` and included in the data pipeline `pipeline.py`\n",
    "<br><br>\n",
    "The two data quality checks performed were:\n",
    "    <ol type = \"1\">\n",
    "         <li>`count_immigration_staging` This quality check ensures that immigration data has been copied from the S3 buckets into Redshift</li>\n",
    "         <li>`count_immigration_fact` This quality check ensures that immigration data has been extracted from the staging table into the immigration fact table</li>\n",
    "      </ol>\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Dictionary\n",
    "\n",
    "#### dim_ports <br>\n",
    "\n",
    "|Field|Type|Description|\n",
    "|----|-----|-----------|\n",
    "|port_id|bigint|Primary Key|\n",
    "|port_code|varchar(3) not null|3 character code used for I94 ports|\n",
    "|port_city|varchar(256)| U.S. city of port|\n",
    "|port_state|varchar(50)|U.S. state of port|\n",
    "|average_temperature|numeric(16,3)|Average temperature of port city|\n",
    "\n",
    "## dim_countries\n",
    "\n",
    "|Field|Type|Description|\n",
    "|----|-----|-----------|\n",
    "|country_id|int8|Primary Key|\n",
    "|country_code|varchar(3) not null|3 character code used for I94 countries|\n",
    "|country|varchar(256) not null|Country from I94 countries|\n",
    "|average_temperature|numeric(16,3)|Average temperature of country|\n",
    "\n",
    "## dim_time\n",
    "\n",
    "|Field|Type|Description|\n",
    "|----|-----|-----------|\n",
    "|sas_timestamp|int not null| Primary Key - The SAS timestamp (days since 1/1/1960)|\n",
    "|year|int not null|4 digit year|\n",
    "|month|int not null|Month (1-12)|\n",
    "|day|int not null|Day (1-31)|\n",
    "|week|int not null|Week of Year (1-52)|\n",
    "|day_of_week|int not null|Day of Week (1-7) starting on Sunday|\n",
    "|quarter|int not null|Quarter of Year (1-4)|\n",
    "\n",
    "## dim_demographics\n",
    "\n",
    "|Field|Type|Description|\n",
    "|----|-----|-----------|\n",
    "|demographics_id|int8|Primary Key|\n",
    "|port_id|int8|Foreign key to dim_ports|\n",
    "|median_age|numeric(18,2)|The median age for the demographic|\n",
    "|male_population|int|Count of male population for city|\n",
    "|female_population|int|Count of female population for city|\n",
    "|total_population|bigint|Count of population for city|\n",
    "|num_of_veterans|int|Count of veterans|\n",
    "|foreign_born|int|Count of foreign born persons|\n",
    "|avg_household_size|numeric(18,2)|Average household size in city|\n",
    "|race|varchar(100)|Race for this demographic|\n",
    "|demo_count|int|Count for this demographic|\n",
    "\n",
    "## dim_airports\n",
    "\n",
    "|Field|Type|Description|\n",
    "|----|-----|-----------|\n",
    "|airport_id|int|Primary Key|\n",
    "|port_id|int|Foreign key to dim_ports|\n",
    "|airport_type|varchar(256)|Short description of airport type|\n",
    "|airport_name|varchar(256)|Airport Name|\n",
    "|elevation_ft|int|Airport elevation|\n",
    "|municipality|varchar(256)|Airport municipality|\n",
    "|gps_code|varchar(256)|Airport GPS code|\n",
    "|iata_code|varchar(256)|Airport International Air Transport Association code|\n",
    "|local_code|varchar(256)|Airport local code|\n",
    "|coordinates|varchar(256)|Airport Coordinates|\n",
    "\n",
    "## fact_immigration\n",
    "\n",
    "|Field|Type|Description|\n",
    "|----|-----|-----------|\n",
    "|immigration_id|bigint|Primary Key|\n",
    "|country_id|bigint|Foreign key to dim_countries|\n",
    "|port_id|bigint|Foreign Key to dim_ports|\n",
    "|age|int|Age of immigrant|\n",
    "|travel_mode|varchar(100)|Mode of travel for immigrant (air, sea, land, etc.)|\n",
    "|visa_category|varchar(100)|Immigrant VISA category|\n",
    "|visa_type|varchar(100)|Type of VISA|\n",
    "|gender|varchar(10)|Immigrant gender|\n",
    "|arrdate|int|SAS timestamp of arrival date, Foreign key to dim_time|\n",
    "|depdate|int|SAS timestamp of departure date, Foreign key to dim_time|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>\n",
    "    I decided to use PySpark to handle the raw data so that if the data grew, this would still be functional. Pyspark was used to clean and preprocess the data. For the DataLake, I used S3 in order to easily use and interact with Redshift. Once the data was loaded into S3, I used Redshift since it was easily compatible with S3. To run the pipeline, I simply used python to manually run the steps of the pipeline. <br><br> </font>\n",
    "    \n",
    "<font color='red'>\n",
    "    This data could be updated daily since it is partitioned by year, month and day. Although, if a backfill needed to occur for specific years then the data could also be updated every year. <br><br>\n",
    "    </font>\n",
    "    \n",
    "<font color='red'>\n",
    "    Different approaches:\n",
    "    <ol>\n",
    "        <li>If the data was increased by 100x, I would scale up my Redshift cluster in order to hold all of the data. I would also load the data directly to S3 and then preprocess and clean the data in S3 to avoid holding all of the data locally</lu>\n",
    "        <li>If the data needed to be updated on a daily basis every day by 7am, I would consider scheduling tools such as Airflow in order to automatically run the pipeline daily</lu>\n",
    "        <li>If the database needed to be accessed by 100+ people then I believe that Redshift would be able to support this requirement as is\n",
    "            </ol>\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
